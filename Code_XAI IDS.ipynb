{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4faf927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Loading\n",
    "file_paths = [\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "              \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "              \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "              \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "              \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "              \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "              \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "              \"Wednesday-workingHours.pcap_ISCX.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ae84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate individual dataframes into one\n",
    "combined_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinity or very large values with NaN\n",
    "combined_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Check for missing values after replacing infinity or very large values\n",
    "print(combined_df.isnull().sum())\n",
    "# Drop rows with missing values\n",
    "combined_df.dropna(inplace=True)\n",
    "# Description of Data:\n",
    "describe_info = combined_df.describe()\n",
    "\n",
    "# Identification of All Zero Columns:\n",
    "all_zeroes_cols = describe_info.loc[:,(describe_info.iloc[1:] == 0).all()]\n",
    "\n",
    "# Dropping All Zero Columns:\n",
    "combined_df.drop(columns=all_zeroes_cols, inplace=True)\n",
    "\n",
    "# Checking Resulting DataFrame's Validity:\n",
    "combined_df.shape  # check if resulting DataFrame valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b25dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of all columns\n",
    "data_types = combined_df.dtypes\n",
    "print(data_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-printable characters and strip leading/trailing spaces\n",
    "\n",
    "combined_df.loc[:,\" Label\"].replace({\"Web Attack � XSS\" : \"XSS\", \"Web Attack � Sql Injection\": \"Sql Injection\", \"Web Attack � Brute Force\": \"Brute Force\"}, inplace=True)\n",
    "# Print unique values after cleaning\n",
    "combined_df[' Label'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dced1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba7291",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot boxplots for numerical features\n",
    "plt.figure(figsize=(16, 10))  # Increase figure size\n",
    "sns.boxplot(data=combined_df)\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "# Save the plot to a file with adjusted dimensions\n",
    "plt.tight_layout()  # Adjust layout to fit labels\n",
    "plt.savefig('outlier.png', dpi=300)  # Increase resolution if needed\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate z-scores for each numerical feature\n",
    "from scipy import stats\n",
    "\n",
    "z_scores = stats.zscore(combined_df.select_dtypes(include=np.number))\n",
    "\n",
    "# Threshold for identifying outliers (e.g., z-score > 3)\n",
    "threshold = 3\n",
    "\n",
    "# Create a boolean mask to identify outliers\n",
    "outlier_mask = np.abs(z_scores) > threshold\n",
    "\n",
    "# Filter out outliers from the DataFrame\n",
    "cleaned_df = combined_df[~outlier_mask.any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758538d5",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b865719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xdf =cleaned_df.drop(' Label', axis=1)\n",
    "ydf =cleaned_df[\" Label\"]\n",
    "\n",
    "# Get the column names before MinMax normalization\n",
    "column_names = xdf.columns\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "dense_array = xdf.astype('int16').values\n",
    "\n",
    "# Now you can use 'dense_array' where dense data is required\n",
    "scaler = MinMaxScaler()\n",
    "# Apply MinMaxScaler directly to the numeric DataFrame\n",
    "xdf = scaler.fit_transform(dense_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb0981",
   "metadata": {},
   "source": [
    "## Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = ydf.value_counts()\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='center')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('before_balancing_label_distribution.png', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93674703",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Print class distribution before balancing\n",
    "class_distribution_before = ydf.value_counts()\n",
    "print(\"Class Distribution before balancing:\")\n",
    "print(class_distribution_before)\n",
    "\n",
    "# Remove classes with too few samples (for example, less than 5 samples)\n",
    "min_samples_threshold = 5\n",
    "filtered_classes = class_distribution_before[class_distribution_before >= min_samples_threshold].index\n",
    "xdf_filtered = xdf[ydf.isin(filtered_classes)]\n",
    "ydf_filtered = ydf[ydf.isin(filtered_classes)]\n",
    "\n",
    "# Print the filtered class distribution\n",
    "print(\"Class Distribution after filtering:\")\n",
    "print(ydf_filtered.value_counts())\n",
    "\n",
    "# Determine the minimum number of samples across all remaining classes for under-sampling\n",
    "min_samples = ydf_filtered.value_counts().min()\n",
    "\n",
    "# Adjust the sampling_strategy for RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy={class_label: min_samples for class_label in ydf_filtered.unique()}, random_state=42)\n",
    "\n",
    "# Create a reasonable sampling strategy for SMOTE based on existing classes\n",
    "smote_sampling_strategy = {class_label: 10000 for class_label in ydf_filtered.unique()}\n",
    "\n",
    "# Reduce the k_neighbors parameter for SMOTE to 1 for classes with less than 6 samples\n",
    "smote = SMOTE(sampling_strategy=smote_sampling_strategy, k_neighbors=1, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([('undersample', rus), ('oversample', smote)])\n",
    "\n",
    "# Apply the pipeline to your filtered data\n",
    "xdf_balanced, ydf_balanced = pipeline.fit_resample(xdf_filtered, ydf_filtered)\n",
    "\n",
    "# Check the class distribution after balancing\n",
    "class_distribution_after = ydf_balanced.value_counts()\n",
    "print(\"\\nClass Distribution after balancing:\")\n",
    "print(class_distribution_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4cabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the balanced dataset\n",
    "print(\"Shape of balanced dataset:\", xdf_balanced.shape, ydf_balanced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ade38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the balanced dataset\n",
    "print(\"Shape of imbalanced dataset:\", xdf.shape, ydf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9ee98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = ydf_balanced.value_counts()\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Distribution of Labels after SMOTE')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('after_balancing_label_distribution.png', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdf_balanced, ydf_balanced, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22300b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab42fa0",
   "metadata": {},
   "source": [
    "## Feature Selection RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d20fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform k-fold cross-validation with k = 5\n",
    "cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Random Forest classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Rank the importance of features\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_features = column_names[sorted_indices]\n",
    "\n",
    "# Print feature importance rankings\n",
    "print(\"Feature Importance Rankings:\")\n",
    "for i, feature in enumerate(sorted_features):\n",
    "    print(f\"{i+1}. {feature}: {feature_importances[sorted_indices[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected feature names based on ranking\n",
    "print(\"Selected Feature (RF) Names Based on Ranking:\")\n",
    "for i, feature_name in enumerate(sorted_features[:10], start=1):\n",
    "    print(f\"{i}. {feature_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677054bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the top 10 feature names and their importance values\n",
    "top_features_rf_names = sorted_features[:10]\n",
    "top_features_rf_importance = feature_importances[:10]\n",
    "\n",
    "# Plot a pie chart for feature importance\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.pie(top_features_rf_importance, labels=top_features_rf_names, autopct='%1.1f%%')\n",
    "plt.title('Feature Importance Ranking')\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('Featur_Importance.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9d9bb",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29070d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 10 features based on importance\n",
    "\n",
    "top_features = sorted_features[:10]\n",
    "\n",
    "# Filter the training and testing data to include only the top features\n",
    "X_train_top = X_train[:, sorted_indices[:10]]\n",
    "X_test_top = X_test[:, sorted_indices[:10]]\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier_top = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation with k = 5\n",
    "cv_scores_top = cross_val_score(rf_classifier_top, X_train_top, y_train, cv=5)\n",
    "\n",
    "# Print the cross-validation scores for the top features\n",
    "print(\"Cross-validation scores with top 10 features:\", cv_scores_top)\n",
    "\n",
    "# Fit the Random Forest classifier on the training data with top features\n",
    "rf_classifier_top.fit(X_train_top, y_train)\n",
    "\n",
    "# Predict labels for the test set with top features\n",
    "y_pred_top = rf_classifier_top.predict(X_test_top)\n",
    "\n",
    "# Calculate accuracy score with top features\n",
    "accuracy_top = accuracy_score(y_test, y_pred_top)\n",
    "print(\"Accuracy with top 10 features:\", accuracy_top)\n",
    "\n",
    "# Generate classification report with top features\n",
    "print(\"Classification Report with top 10 features:\")\n",
    "print(classification_report(y_test, y_pred_top))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef491bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report with top features\n",
    "report = classification_report(y_test, y_pred_top, target_names=rf_classifier_top.classes_, output_dict=True)\n",
    "\n",
    "# Print total accuracy, precision, recall, and F1-score\n",
    "print(\"Total Accuracy:\", report['accuracy'])\n",
    "print(\"Total Precision:\", report['macro avg']['precision'])\n",
    "print(\"Total Recall:\", report['macro avg']['recall'])\n",
    "print(\"Total F1-Score:\", report['macro avg']['f1-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate confusion matrix for Random Forest\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_top)\n",
    "\n",
    "# Plot confusion matrix for Random Forest\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=rf_classifier_top.classes_, \n",
    "            yticklabels=rf_classifier_top.classes_)\n",
    "plt.title('Random Forest')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot for Random Forest\n",
    "plt.savefig('RF_confusion_matrix.png')\n",
    "\n",
    "# Show plot for Random Forest\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the index of the instance you want to check\n",
    "instance_index = 2  # Change this index to the instance you want to check\n",
    "\n",
    "# Get the features and actual label of the instance\n",
    "instance_features = X_test_top[instance_index].reshape(1, -1)\n",
    "actual_label = y_test.iloc[instance_index]\n",
    "\n",
    "# Predict the label for the instance using the trained model\n",
    "predicted_label = rf_classifier_top.predict(instance_features)[0]\n",
    "\n",
    "# Check if the predicted label matches the actual label\n",
    "prediction_correct = predicted_label == actual_label\n",
    "\n",
    "# Print the result\n",
    "if prediction_correct:\n",
    "    print(\"Prediction is correct for instance\", instance_index)\n",
    "    print(\"Actual label:\", actual_label)\n",
    "    print(\"Predicted label:\", predicted_label)\n",
    "else:\n",
    "    print(\"Prediction is incorrect for instance\", instance_index)\n",
    "    print(\"Actual label:\", actual_label)\n",
    "    print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "# Initialize the explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_top,\n",
    "    feature_names=top_features,\n",
    "    class_names=['BENIGN', 'DDOS'],  # Adjust the class names as per your labels\n",
    "    discretize_continuous=True\n",
    ")\n",
    "observation_id = 1  # Adjust this to the specific ID you want to explain\n",
    "observation = X_test_top[observation_id].reshape(1, -1)\n",
    "# Explain the model's prediction on the selected instance\n",
    "explanation = explainer.explain_instance(\n",
    "    observation.flatten(),  # The instance to explain\n",
    "    rf_classifier_top.predict_proba,  # The prediction function\n",
    "    num_features=10  # Number of features to show in explanation\n",
    ")\n",
    "# Show the explanation in the notebook\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5e236",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa23902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Tree classifier on the training data\n",
    "dt_classifier.fit(X_train_top, y_train)\n",
    "\n",
    "# Predict labels for the test set using the trained Decision Tree classifier\n",
    "y_pred_dt = dt_classifier.predict(X_test_top)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Accuracy with Decision Tree:\", accuracy_dt)\n",
    "\n",
    "# Generate classification report for the Decision Tree classifier\n",
    "print(\"Classification Report for Decision Tree:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report with top features\n",
    "dt_report = classification_report(y_test, y_pred_dt, target_names=dt_classifier.classes_, output_dict=True)\n",
    "\n",
    "# Print total accuracy, precision, recall, and F1-score\n",
    "print(\"Total Accuracy:\", dt_report['accuracy'])\n",
    "print(\"Total Precision:\", dt_report['macro avg']['precision'])\n",
    "print(\"Total Recall:\", dt_report['macro avg']['recall'])\n",
    "print(\"Total F1-Score:\", dt_report['macro avg']['f1-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd18074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Select the index of the instance you want to check\n",
    "dt_instance_index = 0  # Change this index to the instance you want to check\n",
    "\n",
    "# Get the features and actual label of the instance\n",
    "dt_instance_features = np.expand_dims(X_test_top[dt_instance_index], axis=0)\n",
    "dt_actual_label = y_test.iloc[dt_instance_index]\n",
    "\n",
    "# Predict the label for the instance using the trained model\n",
    "dt_predicted_label = dt_classifier.predict(dt_instance_features)[0]\n",
    "\n",
    "# Check if the predicted label matches the actual label\n",
    "dt_prediction_correct = dt_predicted_label == dt_actual_label\n",
    "\n",
    "# Print the result\n",
    "if dt_prediction_correct:\n",
    "    print(\"Prediction is correct for instance\", dt_instance_index)\n",
    "    print(\"Actual label:\", dt_actual_label)\n",
    "    print(\"Predicted label:\", dt_predicted_label)\n",
    "else:\n",
    "    print(\"Prediction is incorrect for instance\", dt_instance_index)\n",
    "    print(\"Actual label:\", dt_actual_label)\n",
    "    print(\"Predicted label:\", dt_predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8434f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get unique class labels\n",
    "classes = np.unique(y_test)\n",
    "\n",
    "# Calculate confusion matrix for Decision Tree\n",
    "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# Plot confusion matrix for Decision Tree\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, \n",
    "            yticklabels=classes)\n",
    "plt.title('Decision Tree')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot for Decision Tree\n",
    "plt.savefig('DT_confusion_matrix.png')\n",
    "\n",
    "# Show plot for Decision Tree\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb01738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Initialize the LIME explainer for Decision Tree\n",
    "dt_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_top,  # Training data with top features for Decision Tree\n",
    "    feature_names=top_features,  # Top feature names for Decision Tree\n",
    "    class_names=['BENIGN', 'DDOS'],  # Class names (adjust as per your labels)\n",
    "    discretize_continuous=True  # Discretize continuous features\n",
    ")\n",
    "\n",
    "# Select the observation ID you want to explain for Decision Tree\n",
    "dt_observation_id = 1  # Adjust this to the specific ID you want to explain\n",
    "\n",
    "# Get the observation to explain for Decision Tree\n",
    "dt_observation = X_test_top[dt_observation_id].reshape(1, -1)\n",
    "\n",
    "# Explain the model's prediction on the selected instance for Decision Tree\n",
    "dt_explanation = dt_explainer.explain_instance(\n",
    "    dt_observation.flatten(),  # Instance to explain\n",
    "    dt_classifier.predict_proba,  # Prediction function (Decision Tree)\n",
    "    num_features=10  # Number of features to show in explanation\n",
    ")\n",
    "\n",
    "# Show the explanation in the notebook for Decision Tree\n",
    "dt_explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c842a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f8026b",
   "metadata": {},
   "source": [
    "# SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, probability=True)  # You can adjust kernel and C parameter as needed\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm_classifier.fit(X_train_top, y_train)\n",
    "\n",
    "# Predict labels for the test set using the trained SVM classifier\n",
    "y_pred_svm = svm_classifier.predict(X_test_top)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy with SVM:\", accuracy_svm)\n",
    "\n",
    "# Generate classification report for the SVM classifier\n",
    "print(\"Classification Report for SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd88182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the index of the instance you want to check\n",
    "svm_instance_index = 70  # Change this index to the instance you want to check\n",
    "\n",
    "# Get the features and actual label of the instance\n",
    "svm_instance_features = X_test_top[svm_instance_index].reshape(1, -1)\n",
    "svm_actual_label = y_test.iloc[svm_instance_index]\n",
    "\n",
    "# Predict the label for the instance using the trained model\n",
    "svm_predicted_label = svm_classifier.predict(svm_instance_features)[0]\n",
    "\n",
    "# Check if the predicted label matches the actual label\n",
    "svm_prediction_correct = svm_predicted_label == svm_actual_label\n",
    "\n",
    "# Print the result\n",
    "if svm_prediction_correct:\n",
    "    print(\"Prediction is correct for instance\", svm_instance_index)\n",
    "    print(\"Actual label:\", svm_actual_label)\n",
    "    print(\"Predicted label:\", svm_predicted_label)\n",
    "else:\n",
    "    print(\"Prediction is incorrect for instance\", svm_instance_index)\n",
    "    print(\"Actual label:\", svm_actual_label)\n",
    "    print(\"Predicted label:\", svm_predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c575b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report with top features\n",
    "svm_report = classification_report(y_test, y_pred_svm, target_names=svm_classifier.classes_, output_dict=True)\n",
    "\n",
    "# Print total accuracy, precision, recall, and F1-score\n",
    "print(\"Total Accuracy:\",svm_report['accuracy'])\n",
    "print(\"Total Precision:\", svm_report['macro avg']['precision'])\n",
    "print(\"Total Recall:\", svm_report['macro avg']['recall'])\n",
    "print(\"Total F1-Score:\", svm_report['macro avg']['f1-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e702716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Plot confusion matrix for SVM\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=svm_classifier.classes_, \n",
    "            yticklabels=svm_classifier.classes_)\n",
    "plt.title('SVM')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot for SVM\n",
    "plt.savefig('SVM_confusion_matrix.png')\n",
    "\n",
    "# Show plot for SVM\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Initialize the LIME explainer for SVM\n",
    "svm_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_top,  # Training data with top features for SVM\n",
    "    feature_names=top_features,  # Top feature names for SVM\n",
    "    class_names=['BENIGN', 'DDOS'],  # Class names (adjust as per your labels)\n",
    "    discretize_continuous=True  # Discretize continuous features\n",
    ")\n",
    "\n",
    "# Select the observation ID you want to explain for SVM\n",
    "svm_observation_id = 1  # Adjust this to the specific ID you want to explain\n",
    "\n",
    "# Get the observation to explain for SVM\n",
    "svm_observation = X_test_top[svm_observation_id].reshape(1, -1)\n",
    "\n",
    "# Explain the model's prediction on the selected instance for SVM\n",
    "svm_explanation = svm_explainer.explain_instance(\n",
    "    svm_observation.flatten(),  # Instance to explain\n",
    "    svm_classifier.predict_proba,  # Prediction function (SVM)\n",
    "    num_features=10  # Number of features to show in explanation\n",
    ")\n",
    "\n",
    "# Show the explanation in the notebook for SVM\n",
    "svm_explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed6d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f135bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a13f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59be88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfe21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "418d7602",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the individual classifiers\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "rf_classifier_top = RandomForestClassifier(random_state=42)\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, probability=True)  # Enable probability estimation for SVM\n",
    "\n",
    "# List of (name, estimator) tuples for VotingClassifier\n",
    "estimators = [\n",
    "    ('decision_tree', dt_classifier),\n",
    "    ('random_forest', rf_classifier_top ),\n",
    "    ('svm', svm_classifier)\n",
    "]\n",
    "\n",
    "# Initialize the VotingClassifier\n",
    "voting_classifier = VotingClassifier(estimators, voting='soft')  # You can adjust voting method as needed\n",
    "\n",
    "# Train the VotingClassifier on the training data\n",
    "voting_classifier.fit(X_train_top, y_train)\n",
    "\n",
    "# Predict labels for the test set using the trained VotingClassifier\n",
    "y_pred_voting = voting_classifier.predict(X_test_top)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(\"Accuracy with Voting Classifier:\", accuracy_voting)\n",
    "\n",
    "# Generate classification report for the VotingClassifier\n",
    "print(\"Classification Report for Voting Classifier:\")\n",
    "print(classification_report(y_test, y_pred_voting))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report with top features\n",
    "voting_report = classification_report(y_test, y_pred_voting, target_names=voting_classifier.classes_, output_dict=True)\n",
    "\n",
    "# Print total accuracy, precision, recall, and F1-score\n",
    "print(\"Total Accuracy:\",voting_report['accuracy'])\n",
    "print(\"Total Precision:\", voting_report['macro avg']['precision'])\n",
    "print(\"Total Recall:\", voting_report['macro avg']['recall'])\n",
    "print(\"Total F1-Score:\", voting_report['macro avg']['f1-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b641fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate confusion matrix for Voting Classifier\n",
    "conf_matrix_voting = confusion_matrix(y_test, y_pred_voting)\n",
    "\n",
    "# Plot confusion matrix for Voting Classifier\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_voting, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=voting_classifier.classes_, \n",
    "            yticklabels=voting_classifier.classes_)\n",
    "plt.title('Voting Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot for Voting Classifier\n",
    "plt.savefig('Voting_confusion_matrix.png')\n",
    "\n",
    "# Show plot for Voting Classifier\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Initialize the LIME explainer for SVM\n",
    "voting_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_top,  # Training data with top features for SVM\n",
    "    feature_names=top_features,  # Top feature names for SVM\n",
    "    class_names=['BENIGN', 'DDOS'],  # Class names (adjust as per your labels)\n",
    "    discretize_continuous=True  # Discretize continuous features\n",
    ")\n",
    "\n",
    "# Select the observation ID you want to explain for SVM\n",
    "voting_observation_id = 1  # Adjust this to the specific ID you want to explain\n",
    "\n",
    "# Get the observation to explain for voting\n",
    "voting_observation = X_test_top[voting_observation_id].reshape(1, -1)\n",
    "\n",
    "# Explain the model's prediction on the selected instance for SVM\n",
    "voting_explanation = voting_explainer.explain_instance(\n",
    "   voting_observation.flatten(),  # Instance to explain\n",
    "    voting_classifier.predict_proba,  # Prediction function (SVM)\n",
    "    num_features=10  # Number of features to show in explanation\n",
    ")\n",
    "\n",
    "# Show the explanation in the notebook for voting\n",
    "voting_explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06258e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_explanation.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12647fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7d5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
